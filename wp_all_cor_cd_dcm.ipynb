{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Davide Aloi - PhD student - University of Birmingham\n",
    "# Description: the script correlates current density values within M1 and Th for each \n",
    "# participant with their respective DCM values.\n",
    "\n",
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from nilearn import image, plotting\n",
    "from nilearn.image import new_img_like\n",
    "from scipy import ndimage\n",
    "import scipy.io\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FDR adjusted P value \n",
    "def fdr(p_vals):\n",
    "\n",
    "    from scipy.stats import rankdata\n",
    "    ranked_p_values = rankdata(p_vals)\n",
    "    fdr = p_vals * len(p_vals) / ranked_p_values\n",
    "    fdr[fdr > 1] = 1\n",
    "\n",
    "    return fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters and variables: \n",
    "results_folder = 'D:\\\\roast-chapter3\\\\wp_all_results\\\\' # Folder with results\n",
    "main_folder = 'C:\\\\Users\\\\davide\\\\Documents\\\\GitHub\\\\wp1_2_roast\\\\' # Project folder\n",
    "\n",
    "# Datasets names and subjects lists\n",
    "db_names = ['wp2a', 'wp2a', 'wp1a', 'wp1b']\n",
    "\n",
    "# dataset names for the dcm results (pairwise int)\n",
    "db_names_dcm = ['wp2a_day1_pairwise','wp2a_day5_pairwise', 'wp1a_pairwise', 'wp1b_pairwise']\n",
    "dcm_results_folder = 'C:\\\\Users\\\\davide\\\\Documents\\\\GitHub\\\\wp1_2_roast\\\\all_dcm_results\\\\' # Folder with dcm results\n",
    "\n",
    "db_subjects = [['01','02','03','04','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','22','23','24'], # Wp2a\n",
    "               ['03','04','05','07','09','10','11','12','13','15','16','17','18','19','20','21','22','23','24','25','26'], # Wp1a\n",
    "               ['01','02','03','04','05','06','07','08','09','10','11','12','13','15','16','17','18','19','21','22','23']] # Wp1b                              \n",
    "\n",
    "## Loading AAL3 atlas and extracting M1 / Thalamus ROIs (regions of interest)\n",
    "# AAL3 atlas paper: https://www.oxcns.org/papers/607%20Rolls%20Huang%20Lin%20Feng%20Joliot%202020%20AAL3.pdf \n",
    "AAl3_path = os.path.join(main_folder, 'rois', 'AAL3v1_1mm.nii')\n",
    "AAL3_atlas = image.load_img(AAl3_path)\n",
    "\n",
    "## Creating M1, Th and cerebellar masks from the AAL3 atlas. Load MNI template.\n",
    "# AAL3 index for left M1 = 1\n",
    "m1 = image.math_img(\"np.where(img == 1, 1, 0)\", img = AAL3_atlas) \n",
    "# AAL3 index for TH = 121 - 149 (odd values only (left thalamus)) --> I'm not convinced about this one, ask Davinia\n",
    "th = image.math_img(\"np.where(np.isin(img, np.arange(121, 150, 2)), 1, 0)\", img = AAL3_atlas)\n",
    "# AAL3 index for right Cerebellum (cb) (102 and 108: cerebellar lobes IV-V and VIII)\n",
    "cb = image.math_img(\"np.where(np.isin(img, np.array([102, 108])), 1, 0)\", img = AAL3_atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysing dataset wp2a - N. subjects: 22\n",
      "Correlating  m1  with connection:  m1m1\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n",
      "0.0\n",
      "mean r: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\davide\\Documents\\GitHub\\wp1_2_roast\\wp_all_cor_cd_dcm.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/davide/Documents/GitHub/wp1_2_roast/wp_all_cor_cd_dcm.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m cormap_r[idx,idy,idz], cormap_p[idx,idy,idz] \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mpearsonr(p_v, con)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/davide/Documents/GitHub/wp1_2_roast/wp_all_cor_cd_dcm.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmean r: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/davide/Documents/GitHub/wp1_2_roast/wp_all_cor_cd_dcm.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39;49mmedian(cormap_r))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/davide/Documents/GitHub/wp1_2_roast/wp_all_cor_cd_dcm.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# this is just a check I was doing but you can ignore it \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/davide/Documents/GitHub/wp1_2_roast/wp_all_cor_cd_dcm.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mif\u001b[39;00m previous_p \u001b[39m==\u001b[39m cormap_p[idx,idy,idz]:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmedian\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\davide\\miniconda3\\envs\\neuroimg\\lib\\site-packages\\numpy\\lib\\function_base.py:3793\u001b[0m, in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3711\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_median_dispatcher)\n\u001b[0;32m   3712\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmedian\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, overwrite_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   3713\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3714\u001b[0m \u001b[39m    Compute the median along the specified axis.\u001b[39;00m\n\u001b[0;32m   3715\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3791\u001b[0m \n\u001b[0;32m   3792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3793\u001b[0m     r, k \u001b[39m=\u001b[39m _ureduce(a, func\u001b[39m=\u001b[39;49m_median, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout,\n\u001b[0;32m   3794\u001b[0m                     overwrite_input\u001b[39m=\u001b[39;49moverwrite_input)\n\u001b[0;32m   3795\u001b[0m     \u001b[39mif\u001b[39;00m keepdims:\n\u001b[0;32m   3796\u001b[0m         \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mreshape(k)\n",
      "File \u001b[1;32mc:\\Users\\davide\\miniconda3\\envs\\neuroimg\\lib\\site-packages\\numpy\\lib\\function_base.py:3702\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3700\u001b[0m     keepdim \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m a\u001b[39m.\u001b[39mndim\n\u001b[1;32m-> 3702\u001b[0m r \u001b[39m=\u001b[39m func(a, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   3703\u001b[0m \u001b[39mreturn\u001b[39;00m r, keepdim\n",
      "File \u001b[1;32mc:\\Users\\davide\\miniconda3\\envs\\neuroimg\\lib\\site-packages\\numpy\\lib\\function_base.py:3828\u001b[0m, in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3826\u001b[0m         part \u001b[39m=\u001b[39m a\n\u001b[0;32m   3827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3828\u001b[0m     part \u001b[39m=\u001b[39m partition(a, kth, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3830\u001b[0m \u001b[39mif\u001b[39;00m part\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():\n\u001b[0;32m   3831\u001b[0m     \u001b[39m# make 0-D arrays work\u001b[39;00m\n\u001b[0;32m   3832\u001b[0m     \u001b[39mreturn\u001b[39;00m part\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\davide\\miniconda3\\envs\\neuroimg\\lib\\site-packages\\numpy\\core\\fromnumeric.py:758\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     a \u001b[39m=\u001b[39m asanyarray(a)\u001b[39m.\u001b[39mcopy(order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mK\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 758\u001b[0m a\u001b[39m.\u001b[39;49mpartition(kth, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n\u001b[0;32m    759\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Iterate all three datasets\n",
    "from scipy import stats\n",
    "from statsmodels.stats import multitest as mt\n",
    "import scipy.stats as st\n",
    "from nilearn import glm\n",
    "\n",
    "for db_id, db in enumerate(db_names):\n",
    "    ## Loading results for each dataset (current density maps)\n",
    "    cd_maps = image.load_img(os.path.join(results_folder, db + '_all_cd_maps.nii'))\n",
    "    print('\\nAnalysing dataset ' + db + ' - N. subjects: ' + str(cd_maps.shape[3]))\n",
    "\n",
    "    ## Loading DCM results (nb: the unthresholded ones, for the correlation analysis)\n",
    "    DCM = np.load(os.path.join(dcm_results_folder, db_names_dcm[db_id] + '_dcm_unthresholded.npy'))\n",
    "\n",
    "    ## Conversion of diagonal values to Hz + recentering on 0\n",
    "    funcHz = lambda x: (-0.5*(np.exp(x))) + 0.5 if x != 0 else 0\n",
    "\n",
    "    # Positive values now indicate less self inhibition\n",
    "    # Negative values indicate more self inhibition\n",
    "    for pos in range(0, DCM.shape[0]): \n",
    "        diag = np.diagonal(DCM[pos,:,:])\n",
    "        diag_converted = []\n",
    "        for element in diag:\n",
    "            diag_converted.append(funcHz(element))\n",
    "        np.fill_diagonal(DCM[pos,:,:], np.array(diag_converted))\n",
    "    \n",
    "    ## Resampling masks (masks and current density maps need to have the same shape)\n",
    "    m1_resampled = image.resample_to_img(m1, cd_maps, interpolation = 'nearest').get_fdata()\n",
    "    th_resampled = image.resample_to_img(th, cd_maps, interpolation = 'nearest').get_fdata()\n",
    "    cb_resampled = image.resample_to_img(cb, cd_maps, interpolation = 'nearest').get_fdata()\n",
    "    \n",
    "    # List of Rois and connections \n",
    "    rois = [['m1', m1_resampled],\n",
    "            ['th', th_resampled],\n",
    "            ['cb', cb_resampled]]\n",
    "            \n",
    "    cons = [['m1m1', DCM[:,0,0]],\n",
    "            ['m1th', DCM[:,1,0]],\n",
    "            ['thm1', DCM[:,0,1]],\n",
    "            ['thth', DCM[:,1,1]]]\n",
    "\n",
    "    cd_maps_a = cd_maps.get_fdata() # current density data in numpy format (easier to access)\n",
    "    shape = cd_maps.shape \n",
    "    df = shape[3]-1 # degrees of freedom (N-1). Used for the calculation of T-maps.\n",
    "\n",
    "    # For each dataset, we will iterate each ROI, and for each ROI, each connection (4 connections)\n",
    "    for roi_name, roi in rois:\n",
    "         for con_name, con in cons:\n",
    "            print('Correlating ', roi_name, ' with connection: ', con_name)\n",
    "\n",
    "            # Arrays where we'll save our results (We have 1 result per con/roi/db)\n",
    "            cormap_r = np.zeros(m1_resampled.shape) # r\n",
    "            cormap_p = np.zeros(m1_resampled.shape) # p (uncorrected)\n",
    "            \n",
    "            previous_p = []\n",
    "            # Let's now iterate each voxel in the roi (but consider only those == 1)\n",
    "            for i, v in enumerate(roi.flatten()):\n",
    "                if v != 0: # if voxel is not masked in the ROI\n",
    "                    # Extract the 3d coordinates from 1d index\n",
    "                    idx, idy, idz = np.unravel_index(i, (shape[0],shape[1],shape[2]))\n",
    "                    p_v = cd_maps_a[idx,idy,idz,:] # value for that voxel for all participants (1xN array)\n",
    "                    # calculate r and p and store it in the right voxel\n",
    "                    \n",
    "                    cormap_r[idx,idy,idz], cormap_p[idx,idy,idz] = stats.pearsonr(p_v, con)\n",
    "                    # this is just a check I was doing but you can ignore it \n",
    "                    if previous_p == cormap_p[idx,idy,idz]:\n",
    "                        crash()\n",
    "                    else:\n",
    "                        previous_p = cormap_p[idx,idy,idz]\n",
    "\n",
    "            zvals = st.norm.ppf(cormap_p) # converting pval map to z scores\n",
    "            zvals = new_img_like(cd_maps, zvals) # saving z-scores map into nilearn object\n",
    "            \n",
    "            # First approach (using glm.threshold_stats_img from nilearn)\n",
    "            fdr_p = glm.threshold_stats_img(zvals, alpha = 0.05) # calculating fdr map + threshold using nilearn\n",
    "           \n",
    "            fdr_p = fdr_p[0].get_fdata() # from nilearn to numpy array \n",
    "            \n",
    "            # Second approach using statsmodels (stats.multimodels.fdrcorrection)\n",
    "            cormap_p_nans = np.where(roi == 0, np.nan, cormap_p) # Assigning nans to values outside ROI\n",
    "            fdr_p_2 = mt.multipletests(cormap_p_nans.flatten(), alpha=0.05, method='fdr_bh', is_sorted=False)\n",
    "            \n",
    "            # Third approach (this is very slow I'm sure there's a quicker way)\n",
    "            # like above but I'm using a vector with only the voxels from each roi (that is\n",
    "            # without nans). I do this to cross check results from approach 1 and 2.\n",
    "            \n",
    "            # filtering out nans\n",
    "            cormap_p_nonans = cormap_p_nans[~np.isnan(cormap_p_nans)].flatten()  # this removes NaNs\n",
    "            fdr_p_3 = mt.multipletests(cormap_p_nonans, alpha=0.05, method='fdr_bh', is_sorted=False)\n",
    "\n",
    "            # Printing results \n",
    "            print('Number of significant voxels (unc): ', ((cormap_p < 0.05) & (cormap_p > 0)).sum())\n",
    "            print('Number of significant voxels (fdr): ', ((fdr_p < 0.05) & (fdr_p > 0)).sum())\n",
    "\n",
    "            # printing results with the second approach\n",
    "            print('Number of significant voxels (fdr statsmodels): ', fdr_p_2[0].sum())\n",
    "\n",
    "            # The right one is the one below: this excludes nans from the FDR calculation, \n",
    "            # and some correlations are actually significant. Just a few\n",
    "            print('Number of significant voxels (fdr statsmodels no nans): ', fdr_p_3[0].sum())\n",
    "\n",
    "            # if more than 0 voxels survive FDR then save everything into nifti files\n",
    "            if fdr_p_3[0].sum() > 0:\n",
    "                # save map with r values and p values corrected (FDR)\n",
    "                \n",
    "\n",
    "                # Let's save a map with the adjusted p values (FDR statsmodels, no nans = last approach)\n",
    "                # this isn't very straightforward because the array without nans has a different dimension than \n",
    "                # the original scan (as we took only the values inside the ROI to avoid passing NaNs to the fdr function) \n",
    "                fdr_p_map = np.zeros(m1_resampled.flatten().shape)\n",
    "\n",
    "                # We need to iterate the roi, when we find a value in the roi that is = 1, we take the p from the fdr corrected\n",
    "                # p values list and we increment pos_in_array to move to the next p.\n",
    "                pos_in_array = 0\n",
    "                for i, v in enumerate(roi.flatten()):\n",
    "                    if v != 0:\n",
    "                        fdr_p_map[i] = fdr_p_3[1][pos_in_array]\n",
    "                        pos_in_array+=1\n",
    "\n",
    "                # I'll save a map with only the significant p values (only for rois with\n",
    "                # statistically significant correlations between current density and dcm)\n",
    "                fdr_p_map_thresholded = np.where(fdr_p_map < 0.05, fdr_p_map, 0)  \n",
    "                fdr_p_map_thresholded = np.reshape(fdr_p_map_thresholded, m1_resampled.shape)\n",
    "\n",
    "                if fdr_p_3[0].sum() > 0:\n",
    "                    new_img_like(cd_maps, cormap_p).to_filename(db + '_' + roi_name + '_' + con_name + '_p_uncr.nii')\n",
    "                    #new_img_like(cd_maps, fdr_p_map).to_filename(db_names_dcm[db_id] + '_' + roi_name + '_' + con_name + '_p_corFDR.nii')\n",
    "                    new_img_like(cd_maps, fdr_p_map_thresholded).to_filename(db_names_dcm[db_id] + '_' + roi_name + '_' + con_name + '_p_corFDR_thresholded.nii')\n",
    "                    r_map = new_img_like(cd_maps, cormap_r).get_fdata()\n",
    "                    r_thresholded = np.where(new_img_like(cd_maps, fdr_p_map_thresholded).get_fdata() > 0, r_map, 0)\n",
    "                    new_img_like(cd_maps, r_thresholded).to_filename(db_names_dcm[db_id] + '_' + roi_name + '_' + con_name + '_r.nii')\n",
    "                 \n",
    "                    # I also want to plot correlation between the median of the current density\n",
    "                    # for the roi that resulted significant and dcm value\n",
    "                    #masking current density for each subject\n",
    "                    import matplotlib.pyplot as plt \n",
    "                    cd_maps_a_masked = cd_maps_a.copy()\n",
    "                    all_roi_medians = []\n",
    "                    for i in range(0, cd_maps_a.shape[3]):\n",
    "                        cd_maps_a_masked[:,:,:,i] = cd_maps_a[:,:,:,i] * roi\n",
    "                        cd_maps_a_masked[:,:,:,i] = np.where(cd_maps_a_masked[:,:,:,i] == 0, np.nan, cd_maps_a_masked[:,:,:,i])\n",
    "                        all_roi_medians.append(np.nanmedian(cd_maps_a_masked[:,:,:,i]))\n",
    "\n",
    "                    # printing r and p\n",
    "                    r1,p1  = scipy.stats.pearsonr(np.asarray(all_roi_medians), con)\n",
    "                    print('Single subject median in the roi vs DCM values')\n",
    "                    print('r ' +str(r1))\n",
    "                    print('p ' +str(p1))\n",
    "                    from numpy.polynomial.polynomial import polyfit\n",
    "                    x, y = np.asarray(all_roi_medians), con\n",
    "                    b, m = polyfit(x, y, 1)\n",
    "                    plt.rcParams['font.family'] = 'sans-serif'\n",
    "                    plt.rcParams['figure.figsize'] = (16,8)\n",
    "                    plt.rcParams['figure.facecolor'] = 'white'\n",
    "                    plt.scatter(x, y, edgecolors= \"black\")\n",
    "                    plt.xlabel('Current density median ' + roi_name)\n",
    "                    plt.ylabel('DCM ' + con_name)\n",
    "                    plt.plot(x, b + m * x, '--',color='black', linewidth=0.8, alpha = 0.7)\n",
    "                    plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "                    text = 'r = ' + str(round(r1, 5)) + ' \\np = ' + str(round(p1,4))\n",
    "\n",
    "                    #plt.savefig(db + 'cor.png', dpi=300)\n",
    "                    plt.show()\n",
    "\n",
    "                    # This below is all ignored \n",
    "                    # Let's save the 2 maps\n",
    "                    #new_img_like(cd_maps, fdr_p_map).to_filename(db_names_dcm[db_id] + '_' + roi_name + '_' + con_name + '_p_corFDR.nii')\n",
    "                    #new_img_like(cd_maps, fdr_p_map_thresholded).to_filename(db_names_dcm[db_id] + '_' + roi_name + '_' + con_name + '_p_corFDR_thresholded.nii')\n",
    "                    #new_img_like(cd_maps, cormap_t_cor).to_filename(db + '_' + roi_name + '_' + con_name + '_t_cor.nii')\n",
    "                    #new_img_like(cd_maps, cormap_p).to_filename(db + '_' + roi_name + '_' + con_name + '_p_uncr.nii')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a04e8d3cf57eb13e3e424f0af4edd4725046eee0d61fcbf258511c525184dd3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('neuroimg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
