{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Davide Aloi - PhD student - University of Birmingham\n",
    "\n",
    "# Description: the script calculates median and max current density values for all three\n",
    "# datasets (wp2a, wp1a and wp1b), for three ROIs: left M1, left Th and right CB. \n",
    "# generated using ROAST.\n",
    "\n",
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from nilearn import image\n",
    "from nilearn.image import new_img_like\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters and variables: \n",
    "results_folder = 'D:\\\\roast-chapter3\\\\wp_all_results\\\\' # Folder with results\n",
    "main_folder = 'C:\\\\Users\\\\davide\\\\Documents\\\\GitHub\\\\wp1_2_roast\\\\' # Project folder\n",
    "\n",
    "# Datasets names and subjects lists\n",
    "db_names = ['wp2a', 'wp1a', 'wp1b']\n",
    "\n",
    "db_subjects = [['01','02','03','04','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','22','23','24'], # Wp2a\n",
    "               ['03','04','05','07','09','10','11','12','13','15','16','17','18','19','20','21','22','23','24','25','26'], # Wp1a\n",
    "               ['01','02','03','04','05','06','07','08','09','10','11','12','13','15','16','17','18','19','21','22','23']] # Wp1b                              \n",
    "\n",
    "## Loading AAL3 atlas and extracting M1 / Thalamus ROIs (regions of interest)\n",
    "# AAL3 atlas paper: https://www.oxcns.org/papers/607%20Rolls%20Huang%20Lin%20Feng%20Joliot%202020%20AAL3.pdf \n",
    "AAl3_path = os.path.join(main_folder, 'rois', 'AAL3v1_1mm.nii')\n",
    "AAL3_atlas = image.load_img(AAl3_path)\n",
    "\n",
    "## Creating M1, Th and cerebellar masks from the AAL3 atlas. Load MNI template.\n",
    "# AAL3 index for left M1 = 1\n",
    "m1 = image.math_img(\"np.where(img == 1, 1, 0)\", img = AAL3_atlas) \n",
    "# AAL3 index for TH = 121 - 149 (odd values only (left thalamus)) --> I'm not convinced about this one, ask Davinia\n",
    "th = image.math_img(\"np.where(np.isin(img, np.arange(121, 150, 2)), 1, 0)\", img = AAL3_atlas) \n",
    "# AAL3 index for left SMA = 15\n",
    "sma = image.math_img(\"np.where(img == 15, 1, 0)\", img = AAL3_atlas) \n",
    "# AAL3 index for right Cerebellum (cb) (102 and 108: cerebellar lobes IV-V and VIII)\n",
    "cb = image.math_img(\"np.where(np.isin(img, np.array([102, 108])), 1, 0)\", img = AAL3_atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing dataset wp2a - N. subjects: 22\n",
      "    m1_medians    m1_max  th_medians    th_max  cb_medians    cb_max\n",
      "01    0.156922  1.278081    0.085916  0.114896    0.047411  0.103095\n",
      "02    0.121956  1.323492    0.080336  0.121100    0.039152  0.105412\n",
      "03    0.162366  0.926881    0.095327  0.138950    0.035664  0.071599\n",
      "04    0.164472  0.806731    0.110396  0.172870    0.040185  0.093821\n",
      "06    0.193781  0.788048    0.138732  0.187466    0.048805  0.110307\n",
      "Analysing dataset wp1a - N. subjects: 21\n",
      "    m1_medians    m1_max  th_medians    th_max  cb_medians    cb_max\n",
      "03    0.186333  0.748116    0.090573  0.127735    0.024622  0.061537\n",
      "04    0.185302  1.212486    0.104725  0.135747    0.046393  0.102993\n",
      "05    0.187244  0.421784    0.120467  0.150625    0.047443  0.100850\n",
      "07    0.147577  0.778978    0.099740  0.119804    0.045784  0.089133\n",
      "09    0.103404  0.469499    0.067735  0.088661    0.028462  0.058398\n",
      "Analysing dataset wp1b - N. subjects: 21\n",
      "    m1_medians    m1_max  th_medians    th_max  cb_medians    cb_max\n",
      "01    0.037661  0.059230    0.081316  0.122941    0.200450  1.461415\n",
      "02    0.060538  0.095006    0.114796  0.182871    0.257465  0.926864\n",
      "03    0.067794  0.101710    0.119215  0.196144    0.234956  0.568916\n",
      "04    0.070432  0.115579    0.132257  0.205628    0.230320  0.919702\n",
      "05    0.053968  0.074243    0.109457  0.147343    0.206910  1.352553\n"
     ]
    }
   ],
   "source": [
    "# Iterate all datasets\n",
    "for db_id, db in enumerate(db_names):\n",
    "\n",
    "    # Lists where current density values will be stored\n",
    "    all_m1_medians = []\n",
    "    all_th_medians = []\n",
    "    all_sma_medians = []\n",
    "    all_cb_medians = []\n",
    "    all_m1_max = []\n",
    "    all_th_max = []\n",
    "    all_sma_max = []\n",
    "    all_cb_max = []\n",
    "\n",
    "    # Loading results for each dataset (only current density maps)\n",
    "    cd_maps = image.load_img(os.path.join(results_folder, db + '_all_cd_maps.nii'))\n",
    "    #emag_maps = image.load_img(os.path.join(results_folder, str(db + '_all_emag_maps.nii')))\n",
    "    #masks_maps = image.load_img(os.path.join(results_folder, str(db + '_all_masks_maps.nii')))\n",
    "    print('Analysing dataset ' + db + ' - N. subjects: ' + str(cd_maps.shape[3]))\n",
    "    \n",
    "    for i in range(0, len(db_subjects[db_id])): # Iterate all subjects\n",
    "        cd_map = image.index_img(cd_maps, i) # extracting volume of subject i \n",
    "        # which is equal to: cd_maps.get_fdata()[:,:,:,i], but returns a nibabel object\n",
    "\n",
    "        ## Masking\n",
    "        # Resampling masks to cd_maps shape (done only once)\n",
    "        if not 'm1_resampled' in locals():\n",
    "            m1_resampled = image.resample_to_img(m1, cd_maps, interpolation = 'nearest')\n",
    "            th_resampled = image.resample_to_img(th, cd_maps, interpolation = 'nearest')\n",
    "            sma_resampled = image.resample_to_img(sma, cd_maps, interpolation = 'nearest')\n",
    "            cb_resampled = image.resample_to_img(cb, cd_maps, interpolation = 'nearest')\n",
    "\n",
    "        cd_m1 = image.math_img(\"img * img2\", img = cd_map, img2 = m1_resampled) # Applying m1 mask\n",
    "        cd_th = image.math_img(\"img * img2\", img = cd_map, img2 = th_resampled) # Applying th mask\n",
    "        cd_sma = image.math_img(\"img * img2\", img = cd_map, img2 = sma_resampled) # Applying sma mask\n",
    "        cd_cb = image.math_img(\"img * img2\", img = cd_map, img2 = cb_resampled) # Applying cb mask\n",
    "\n",
    "        # Assigning NaNs to values = 0, to exclude non M1, Th or CB voxels from the\n",
    "        # calculation of the median and max values.\n",
    "        cd_m1_nans = np.where(m1_resampled.get_fdata() == 0, np.nan, cd_m1.get_fdata())\n",
    "        cd_th_nans = np.where(th_resampled.get_fdata() == 0, np.nan, cd_th.get_fdata())\n",
    "        cd_sma_nans = np.where(sma_resampled.get_fdata() == 0, np.nan, cd_sma.get_fdata())\n",
    "        cd_cb_nans = np.where(cb_resampled.get_fdata() == 0, np.nan, cd_cb.get_fdata())\n",
    "\n",
    "        ## Calculation of median and max current density values, within each ROI\n",
    "        m1_cd_median = np.nanmedian(cd_m1_nans)\n",
    "        th_cd_median = np.nanmedian(cd_th_nans)\n",
    "        sma_cd_median = np.nanmedian(cd_sma_nans)\n",
    "        cb_cd_median = np.nanmedian(cd_cb_nans)\n",
    "        m1_cd_max = np.nanmax(cd_m1_nans)\n",
    "        th_cd_max = np.nanmax(cd_th_nans)\n",
    "        sma_cd_max = np.nanmax(cd_sma_nans)\n",
    "        cb_cd_max = np.nanmax(cd_cb_nans)\n",
    "\n",
    "        # Saving means, medians and max values into their respective lists   \n",
    "        all_m1_medians.append(m1_cd_median)\n",
    "        all_th_medians.append(th_cd_median)  \n",
    "        all_sma_medians.append(sma_cd_median)\n",
    "        all_cb_medians.append(cb_cd_median) \n",
    "        all_m1_max.append(m1_cd_max)\n",
    "        all_th_max.append(th_cd_max)   \n",
    "        all_sma_max.append(sma_cd_max)\n",
    "        all_cb_max.append(cb_cd_max)\n",
    "\n",
    "    # Saving .csv file with results\n",
    "    dataframe = pd.DataFrame(list(zip(all_m1_medians, all_m1_max, all_th_medians,\n",
    "                                all_th_max, all_sma_medians, all_sma_max, all_cb_medians, all_cb_max)),\n",
    "                                columns = ['m1_medians', 'm1_max', 'th_medians',\n",
    "                                'th_max', 'sma_medians', 'sma_max', 'cb_medians', 'cb_max'],\n",
    "                                index = db_subjects[db_id])\n",
    "    dataframe.to_csv(os.path.join(results_folder, db + '_current_density_results.csv'))\n",
    "    print(dataframe.head()) # Just printing the first few results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a04e8d3cf57eb13e3e424f0af4edd4725046eee0d61fcbf258511c525184dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
